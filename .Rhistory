wssplot(df)
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
}
wssplot(df)
wssplot <- kmeans(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
}
data
wine$Type=NULL
data<-scale(wine)
# Now we'd like to cluster the data using K-Means.
# How do we decide how many clusters to use if you don't know that already?
# We'll try two methods.
# Method 1: A plot of the total within-groups sums of squares against the
# number of clusters in a K-means solution can be helpful. A bend in the
# graph can suggest the appropriate number of clusters.
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
}
wssplot(df)
apply(data,2,var)
dim(data)
data(wine, package="rattle")
head(wine)
(wine[-1])
data<-scale(wine)
data<-scale(wine)
wine
data<-(wine[-1])
data<-scale(data)
data
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
}
wssplot(df)
data(wine, package="rattle")
head(wine)
df <- scale(wine[-1])
wssplot(df)
library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
table(nc$Best.n[1,])
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
set.seed(1234)
fit.km <- kmeans(df, 3, nstart=25)
fit.km$size
data <- scale(wine[-1])
wssplot(df)
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
}
wssplot(df)
library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
fit.km <- kmeans(data, 3)
ct.km <- table(wine$Type, fit.km$cluster)
ct.km
ct.km <- table(wine$Type, fit.km$clusters)
ct.km <- table(wine$Type, fit.km$cluster)
ct.km
clusplot(data, fit$cluster, color=TRUE, shade=TRUE,
labels=2, lines=0)
library(cluster)
install.packages(c("cluster", "rattle","NbClust"))
install.packages(c("cluster", "rattle", "NbClust"))
library(cluster)
clusplot(data, fit$cluster, color=TRUE, shade=TRUE,
labels=2, lines=0)
clusplot(data, fit.km$cluster, color=TRUE, shade=TRUE,
labels=2, lines=0)
clusplot(data, fit.km$cluster, color=TRUE, shade=TRUE,
labels=3, lines=0)
summary(ct.km)
library(flexclust)
randIndex(ct.km)
library(flexclust)
install.packages(c("cluster", "rattle","NbClust", "flexclust"))
install.packages(c("cluster", "rattle", "NbClust", "flexclust"))
library(flexclust)
randIndex(ct.km)
# This mini-project is based on the K-Means exercise from 'R in Action'
# Go here for the original blog post and solutions
# http://www.r-bloggers.com/k-means-clustering-from-r-in-action/
# Exercise 0: Install these packages if you don't have them already
install.packages(c("cluster", "rattle","NbClust", "flexclust"))
# Now load the data and look at the first few rows
data(wine, package="rattle")
head(wine)
# Exercise 1: Remove the first column from the data and scale
# it using the scale() function
data <- scale(wine[-1])
# Now we'd like to cluster the data using K-Means.
# How do we decide how many clusters to use if you don't know that already?
# We'll try two methods.
# Method 1: A plot of the total within-groups sums of squares against the
# number of clusters in a K-means solution can be helpful. A bend in the
# graph can suggest the appropriate number of clusters.
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
}
wssplot(df)
## The plot in Ex 1 suggests 3 clusters - look for the elbow in the curve; where the sharp decrease tails off
# Exercise 2:
#   * How many clusters does this method suggest?
#####  Ans: 3 clusters
#   * Why does this method work? What's the intuition behind it?
###### Ans: Look for the elbow in the curve; where the sharp decrease tails off
######      The first clusters will explain a lot of variation - as more are added, this benefit will tail off eventually
#   * Look at the code for wssplot() and figure out how it works
# Method 2: Use the NbClust library, which runs many experiments
# and gives a distribution of potential number of clusters.
library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
# Exercise 3: How many clusters does this method suggest?
#### Ans: From the resulting barplot, this method also suggests 3 clusters
# Exercise 4: Once you've picked the number of clusters, run k-means
# using this number of clusters. Output the result of calling kmeans()
# into a variable fit.km
fit.km <- kmeans(data, 3)
# Now we want to evaluate how well this clustering does.
# Exercise 5: using the table() function, show how the clusters in fit.km$clusters
# compares to the actual wine types in wine$Type. Would you consider this a good
# clustering?
ct.km <- table(wine$Type, fit.km$cluster)
### Is it a good clustering? (flexcrust?)
library(flexclust)
randIndex(ct.km)
# Ans: To quantify, using an adjusted Rank index; the agreement between type and cluster:
# Ans: On a scale of -1,+1; the value of 0.9 represents a good model.
# Exercise 6:
# * Visualize these clusters using  function clusplot() from the cluster library
# * Would you consider this a good clustering?
#clusplot( ... )
library(cluster)
clusplot(data, fit.km$cluster, color=TRUE, shade=TRUE,
labels=2, lines=0)
## Ans: It appears quite good - membership is largely non-ambiguous
install.packages(c("cluster", "rattle", "NbClust", "flexclust"))
data(wine, package="rattle")
head(wine)
data <- scale(wine[-1])
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
}
wssplot(df)
library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
fit.km <- kmeans(data, 3)
ct.km <- table(wine$Type, fit.km$cluster)
library(flexclust)
randIndex(ct.km)
library(cluster)
clusplot(data, fit.km$cluster, color=TRUE, shade=TRUE,
labels=2, lines=0)
###### Adding Text
### Linearly Inseperable
DF1 <- data.frame(
A=c(2, 3, 3, 4, 4.25, 4.5),
B=c(2, 2, 4, 3, 2, 3),
C=c(1, 1, 2, 2, 1, 2)
)
### Linearly Seperable
DF2 <- data.frame(
A=c(2, 3, 3, 4, 1.5, 4.5),
B=c(2, 2, 4, 3, 3.5, 3),
C=c(1, 1, 2, 2, 1, 2)
)
par(mfrow=c(1,2))
plot(B ~ A,
xlab = "",
ylab = "",
pch = c(16, 17),  # different 'pch' types
main="Linearly Inseperable",
col = c("red","blue"),
data = DF1, cex=1.5, xlim=c(1, 5), ylim=c(1, 5))
plot(B ~ A,
xlab = "",
ylab = "",
pch = c(16, 17),  # different 'pch' types
main="Linearly Seperable",
col = c("red","blue"),
data = DF2, cex=1.5, xlim=c(1, 5), ylim=c(1, 5))
abline(a=-0.6, b=1,col="black")
summary(adult)
library(ggplot2)
summary(adult)
library(ggplot)
library(ggplot2)
summary(adult)
require(ggplot2)
summary(adult)
str(ggplot2)
dir(ggplot2)
setwd("C:/Users/Mark's/Desktop/Data Science/DataScienceAssignments/logistic_regression")
NH11 <- readRDS("dataSets/NatHealth2011.rds")
labs <- attributes(NH11)$labels
str(NH11$hypev) # check stucture of hypev
levels(NH11$hypev) # check levels of hypev
NH11$hypev <- factor(NH11$hypev, levels=c("2 No", "1 Yes"))
levels(NH11$hypev)
hyp.out <- glm(hypev~age_p+sex+sleep+bmi,
data=NH11, family="binomial")
coef(summary(hyp.out))
hyp.out.tab <- coef(summary(hyp.out))
hyp.out.tab[, "Estimate"] <- exp(coef(hyp.out))
hyp.out.tab
els
predDat <- with(NH11,
expand.grid(age_p = c(33, 63),
sex = "2 Female",
bmi = mean(bmi, na.rm = TRUE),
sleep = mean(sleep, na.rm = TRUE)))
cbind(predDat, predict(hyp.out, type = "response",
se.fit = TRUE, interval="confidence",
newdata = predDat))
cbind(predDat, predict(hyp.out, type = "response",
se.fit = TRUE, interval="confidence",
newdata = predDat))
library(effects)
plot(allEffects(hyp.out))
library(effects)
package("effects")
load("effects")
install.packages("effects")
library(effects)
plot(allEffects(hyp.out))
##   Let's predict the probability of being diagnosed with hypertension
##   based on age, sex, sleep, and bmi
## Regression with binary outcomes
## âââââââââââââââââââââââââââââââââ
## Logistic regression
## âââââââââââââââââââââââ
##   This far we have used the `lm' function to fit our regression models.
##   `lm' is great, but limitedâin particular it only fits models for
##   continuous dependent variables. For categorical dependent variables we
##   can use the `glm()' function.
##   For these models we will use a different dataset, drawn from the
##   National Health Interview Survey. From the [CDC website]:
##         The National Health Interview Survey (NHIS) has monitored
##         the health of the nation since 1957. NHIS data on a broad
##         range of health topics are collected through personal
##         household interviews. For over 50 years, the U.S. Census
##         Bureau has been the data collection agent for the National
##         Health Interview Survey. Survey results have been
##         instrumental in providing data to track health status,
##         health care access, and progress toward achieving national
##         health objectives.
##   Load the National Health Interview Survey data:
NH11 <- readRDS("dataSets/NatHealth2011.rds")
labs <- attributes(NH11)$labels
##   [CDC website] http://www.cdc.gov/nchs/nhis.htm
## Logistic regression example
## âââââââââââââââââââââââââââââââ
##   Let's predict the probability of being diagnosed with hypertension
##   based on age, sex, sleep, and bmi
str(NH11$hypev) # check stucture of hypev
levels(NH11$hypev) # check levels of hypev
# collapse all missing values to NA
NH11$hypev <- factor(NH11$hypev, levels=c("2 No", "1 Yes"))
# run our regression model
hyp.out <- glm(hypev~age_p+sex+sleep+bmi,
data=NH11, family="binomial")
coef(summary(hyp.out))
## Logistic regression coefficients
## ââââââââââââââââââââââââââââââââââââ
##   Generalized linear models use link functions, so raw coefficients are
##   difficult to interpret. For example, the age coefficient of .06 in the
##   previous model tells us that for every one unit increase in age, the
##   log odds of hypertension diagnosis increases by 0.06. Since most of us
##   are not used to thinking in log odds this is not too helpful!
##   One solution is to transform the coefficients to make them easier to
##   interpret
hyp.out.tab <- coef(summary(hyp.out))
hyp.out.tab[, "Estimate"] <- exp(coef(hyp.out))
hyp.out.tab
## Generating predicted values
## âââââââââââââââââââââââââââââââ
##   In addition to transforming the log-odds produced by `glm' to odds, we
##   can use the `predict()' function to make direct statements about the
##   predictors in our model. For example, we can ask "How much more likely
##   is a 63 year old female to have hypertension compared to a 33 year old
##   female?".
# Create a dataset with predictors set at desired levels
predDat <- with(NH11,
expand.grid(age_p = c(33, 63),
sex = "2 Female",
bmi = mean(bmi, na.rm = TRUE),
sleep = mean(sleep, na.rm = TRUE)))
# predict hypertension at those levels
cbind(predDat, predict(hyp.out, type = "response",
se.fit = TRUE, interval="confidence",
newdata = predDat))
##   This tells us that a 33 year old female has a 13% probability of
##   having been diagnosed with hypertension, while and 63 year old female
##   has a 48% probability of having been diagnosed.
## Packages for  computing and graphing predicted values
## âââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
##   Instead of doing all this ourselves, we can use the effects package to
##   compute quantities of interest for us (cf. the Zelig package).
install.packages("effects")
library(effects)
plot(allEffects(hyp.out))
install.packages("effects")
str(NH11$hypev) # check stucture of hypev
levels(NH11$hypev) # check levels of hypev
str(NH11$age) # check stucture of hypev
summary(NH11$age) # check stucture of hypev
str(NH11$sex) # check stucture of sex
str(NH11$hypev) # check stucture of hypev
str(NH11$age) # check stucture of age
str(NH11$sex) # check stucture of sex
str(NH11$sleep) # check stucture of sleep
summary(NH11$sleep) # check stucture of sleep
str(NH11$sleep) # check stucture of sleep
summary(NH11$sleep) # check stucture of sleep
summary(NH11$sleep) # check stucture of sleep- continuous
str(NH11$bmi) # check stucture of bmi
summary(NH11$bmi) # check stucture of bmi
hist(NH11$age)
par(mfrow=c(1,1))
hist(NH11$age)
hist(NH11$sleep)
hist(NH11$age)
hist(NH11$sleep)
par(mfrow=c(1,1))
hist(NH11$sleep)
max(NH11$sleep)
hist(NH11$bmi)
max(NH11$bmi)
max(NH11$sleep) # 99 is a missing value
max(NH11$bmi)
hist(NH11$bmi)
hist(NH11$age)  # variable is okay
max(NH11$sleep) # 99 is a missing value
hist(NH11$sleep) # 99 is a missing value
hist(NH11$bmi)
NH11$sleep[NH11$sleep==99] <- NA
hist(NH11$sleep)
max(NH11$sleep)
summary(NH11$sleep)
labs <- attributes(NH11)$labels
labs
str(NH11$bmi) # check stucture of bmi - continuous
hist(NH11$bmi) # Extreme BMI values - valid readings
labs <- attributes(NH11)$labels
labs
hist(NH11$age)  # variable is okay
hist(NH11$age_p)  # variable is okay
hist(NH11$age)  # variable is okay
hist(NH11$sleep) # 99 is a missing value
NH11$sleep[NH11$sleep >=30] <- NA
summary(NH11$sleep)
NH11$sleep[NH11$sleep >=24] <- NA
str(NH11$r_marital)
str(NH11$r_maritl)
head(NH11$r_maritl)
lab(NH11$r_maritl)
levels(NH11$r_maritl)
str(NH11$everwrk)
str(NH11$everwrk)
header(NH11$everwrk) # check stucture of hypev - 10 level factor
head(NH11$everwrk) # check stucture of hypev - 10 level factor
head(NH11$age_p) # check stucture of age - continuous
str(NH11$age_p) # check stucture of age - continuous
max(NH11$age_p) # check stucture of age - continuous
hist(NH11$age_p)
str(NH11$rmaritl) # check stucture of bmi - continuous
str(NH11$r_maritl) # check stucture of bmi - continuous
## Regression with binary outcomes
## âââââââââââââââââââââââââââââââââ
## Logistic regression
## âââââââââââââââââââââââ
##   This far we have used the `lm' function to fit our regression models.
##   `lm' is great, but limitedâin particular it only fits models for
##   continuous dependent variables. For categorical dependent variables we
##   can use the `glm()' function.
##   For these models we will use a different dataset, drawn from the
##   National Health Interview Survey. From the [CDC website]:
##         The National Health Interview Survey (NHIS) has monitored
##         the health of the nation since 1957. NHIS data on a broad
##         range of health topics are collected through personal
##         household interviews. For over 50 years, the U.S. Census
##         Bureau has been the data collection agent for the National
##         Health Interview Survey. Survey results have been
##         instrumental in providing data to track health status,
##         health care access, and progress toward achieving national
##         health objectives.
##   Load the National Health Interview Survey data:
NH11 <- readRDS("dataSets/NatHealth2011.rds")
labs <- attributes(NH11)$labels
##   [CDC website] http://www.cdc.gov/nchs/nhis.htm
## Logistic regression example
## âââââââââââââââââââââââââââââââ
##   Let's predict the probability of being diagnosed with hypertension
##   based on age, sex, sleep, and bmi
str(NH11$hypev) # check stucture of hypev
levels(NH11$hypev) # check levels of hypev
# collapse all missing values to NA
NH11$hypev <- factor(NH11$hypev, levels=c("2 No", "1 Yes"))
# run our regression model
hyp.out <- glm(hypev~age_p+sex+sleep+bmi,
data=NH11, family="binomial")
coef(summary(hyp.out))
## Logistic regression coefficients
## ââââââââââââââââââââââââââââââââââââ
##   Generalized linear models use link functions, so raw coefficients are
##   difficult to interpret. For example, the age coefficient of .06 in the
##   previous model tells us that for every one unit increase in age, the
##   log odds of hypertension diagnosis increases by 0.06. Since most of us
##   are not used to thinking in log odds this is not too helpful!
##   One solution is to transform the coefficients to make them easier to
##   interpret
hyp.out.tab <- coef(summary(hyp.out))
hyp.out.tab[, "Estimate"] <- exp(coef(hyp.out))
hyp.out.tab
## Generating predicted values
## âââââââââââââââââââââââââââââââ
##   In addition to transforming the log-odds produced by `glm' to odds, we
##   can use the `predict()' function to make direct statements about the
##   predictors in our model. For example, we can ask "How much more likely
##   is a 63 year old female to have hypertension compared to a 33 year old
##   female?".
# Create a dataset with predictors set at desired levels
predDat <- with(NH11,
expand.grid(age_p = c(33, 63),
sex = "2 Female",
bmi = mean(bmi, na.rm = TRUE),
sleep = mean(sleep, na.rm = TRUE)))
# predict hypertension at those levels
cbind(predDat, predict(hyp.out, type = "response",
se.fit = TRUE, interval="confidence",
newdata = predDat))
##   This tells us that a 33 year old female has a 13% probability of
##   having been diagnosed with hypertension, while and 63 year old female
##   has a 48% probability of having been diagnosed.
## Packages for  computing and graphing predicted values
## âââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
##   Instead of doing all this ourselves, we can use the effects package to
##   compute quantities of interest for us (cf. the Zelig package).
install.packages("effects")
library(effects)
plot(allEffects(hyp.out))
install.packages("effects")
str(NH11$everwrk) # Factor; 5 Levels; Cleaning Required
str(NH11$age_p) # Check stucture of age - continuous
hist(NH11$age_p) # Looks to be okay
str(NH11$r_maritl) # Factor; 10 Levels; Cleaning Required
str(NH11$hypev) # check stucture of hypev
levels(NH11$hypev) # check levels of hypev
